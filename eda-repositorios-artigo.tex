%%
%% This is file `sample-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%% v
%% samples.dtx  (with options: `authordraft')
%%
%% IMPORTANT NOTICE:
%%
%% For the copyright see the source file.
%%
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-authordraft.tex.
%%
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%%
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.

% \documentclass[sigconf,authordraft]{acmart}
\documentclass[sigconf,nonacm]{acmart}

\usepackage[portuges]{babel}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

\usepackage[acronym]{glossaries}
\makeglossaries

\urlstyle{same}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
\acmBooktitle{Estratégia para os Dados Abertos}
%\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\input{acronimos}

%%
%% end of the preamble, start of the body of the source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Repositórios de Dados: Objetivos, Funcionalidades e Alternativas}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{João Rocha da Silva}
\authornote{Investigador no INESC TEC e Professor Auxiliar Convidado na Faculdade de Engenharia da Universidade do Porto}
\email{joaorosilva@gmail.com}
\orcid{0000-0001-9659-6256}
\affiliation{%
  \institution{INESC TEC / Faculdade de Engenharia da Universidade do Porto}
  \streetaddress{Campus da Faculdade de Engenharia da Universidade do Porto, Rua Dr. Roberto Frias}
  \city{Porto}
  \state{Portugal}
  \postcode{4200-465}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{João Rocha da Silva}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
    Os repositórios de dados de investigação são cada vez mais uma peça essencial para o processo científico. Não só fomentam a reprodutibilidade das conclusões publicadas nos artigos científicos como assumem um papel crucial na atribuição de crédito aos criadores de dados, pois expôem ao público um trabalho de recolha, processamento e anotação que tanto tem de dispensioso como por vezes de invisível.
    A escolha de um software para suportar um repositório de dados deve ser guiada pelas necessidades das principais partes interessadas. Assim sendo, neste artigo discutem-se as principais funcionalidades desejáveis num repositório de dados, tanto do ponto de vista técnico (software e infraestrutura) como do ponto de vista político, nomeadamente no que diz respeito às garantias e compromissos a assumir pelas instituições que os alojam no sentido de permitir a sua certificação de acordo com a estratégia da European Open Science Cloud.
\end{abstract}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{repositórios de dados, ciência aberta, e-Science, Estratégia para os Dados Abertos, Fundação para a Ciência e Tecnologia}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introdução}

Os princípios \gls{FAIR} para a gestão de dados de investigação especificam que os dados devem ser \emph{Findable} (encontráveis), \emph{Accessible} (acessíveis), \emph{Interoperable} (interoperáveis) e \emph{Reusable} (reutilizáveis)~\cite{wilkinson2016fair}. Recentemente, um grupo de peritos da Comissão Europeia estudou as melhores práticas para tornar os dados de investigação europeus mais FAIR, tendo concluído que os componentes essenciais de um ecossistema \gls{FAIR} são as políticas, os planos de gestão de dados (DMP), os identificadores persistentes, as normas, e finalmente os repositórios~\cite{hodson2018turning}.

Sendo esta análise mais focada nos aspetos técnicos ligados às plataformas de software que podem sustentar um repositório, o aspeto político não será a tónica principal; ressalva-se contudo que as decisões políticas devem ser informadas pelas capacidades técnicas das plataformas disponíveis---é aliás nesse sentido que se este artigo se posiciona.

Falando de planos de gestão de dados, assiste-se a uma necessidade crescente de automação dos processos de gestão de dados, motivada pela grande quantidade e diversidade dos dados produzidos pelos investigadores. Os planos de gestão de dados consistem vulgarmente em documentos escritos e destinados a seres humanos, que são acrescentados às propostas de projetos de investigação em certos concursos. Mais recentemente, têm sido desenvolvidos esforços no sentido de os tornar acionáveis por máquinas, dando origem aos \gls{maDMP}. Estes apresentam-se como um documento vivo e interpretável por máquinas, reduzindo assim a necessidade de intervenção humana na execução e auditoria das práticas de gestão de dados a que os investigadores se comprometem aquando da criação do documento. Os repositórios devem portanto caminhar no sentido de suportar as operações necessárias à execução dos \gls{maDMP}.

Os identificadores persistentes, ou \gls{PID}, são uma parte essencial dos metadados acrescentados aos conjuntos de dados, pois permitem a recuperação e identificação uníca de cada conjunto de dados. Qualquer plataforma de repositório deve então oferecer integrações com os fornecedores de \gls{PID}. Há diversos sistemas de identificadores por onde escolher, e essa seleção deve também ser controlada por normas de certificação. A escolha da certificação prestigiante mas também acessível deve ser feita no sentido de aumentar o valor acrescentado para todas as necessidades e os recursos disponíveis das partes interessadas desse repositório. Devem portanto ser tidas em conta as raras mas preciosas histórias de sucesso na certificação aquando da seleção de uma plataforma de software para a montagem de um repositório.

Este artigo contém 5 secções para além desta introdução. A secção~\ref{sec:o_papel_do_repositorio_na_aplicacao_dos_principios_fair} apresenta uma análise de alto nível dos requisitos a satisfazer por um repositório de dados, tendo os princípios \gls{FAIR} como linhas orientadoras.
%
Tendo em conta a crescente importância da automação nos processos de gestão de dados como suporte aos Planos de Gestão de Dados accionáveis por máquinas, analisam-se também esses aspetos na secção~\ref{sec:a_automacao_dos_planos_de_gestao_de_dados}.
%
Na secção~\ref{sec:comparacao_de_funcionalidades} é apresentada uma comparação entre soluções de software para repositórios. Esta foi produzida a partir de um estudo do grupo de trabalho RDA sobre plataformas de dados de investigação e uma comparação feita pela Dataverse, após consulta aos principais fornecedores de repositórios.
%
Dada a importância da certificação no desenho de um \emph{workflow} de gestão de dados, apresenta-se também na secção~\ref{sec:certificacao} uma discussão sobre a evolução de alguns dos esquemas de certificação de repositórios de dados, em linha com as mais recentes recomendações da Comissão Europeia.
%
O artigo termina com a secção~\ref{sec:discussao_e_conclusoes}, onde se apresentam algumas conclusões e discussão sobre a análise levada a cabo, assim como algumas opiniões relativamente ao futuro dos repositórios de dados.

\section{O papel do repositório na aplicação dos princípios FAIR} % (fold)
\label{sec:o_papel_do_repositorio_na_aplicacao_dos_principios_fair}

Os princípios \gls{FAIR} são linhas orientadoras para a melhoria dos processos de gestão de dados, abarcando portanto aspetos bastante mais genéricos do que a tecnologia que suporta um repositório de dados. Assim, e para ajudar a conduzir um artigo bastante focado em tecnologia, utilizaremos os princípios \gls{FAIR} como guiões orientadores para elencar algumas das funcionalidades relevantes de um repositório de dados.

O primeiro dos princípios \gls{FAIR} diz que os dados devem ser \emph{Findable}, ou seja, devem ser fáceis de descobrir, tanto por humanos como por máquinas. Para tal, devem incluir (F1) um identificador global único e persistente, (F2) possuir metadados ricos, que por sua vez (F3) devem incluir claramente o identificador dos recursos que descrevem. O quarto e último aspeto, (F4), especifica que os tanto dados como metadados devem estar registados em recursos pesquisáveis.

Para facilitar a satisfação destes requisitos, uma plataforma de repositório deverá oferecer integração programática com um fornecedor de identificadores persistentes como por exemplo os \gls{DOI} ou handle\footnote{\url{http://www.handle.net/}}. A vantagem para os gestores de repositório é a possibilidade de atribuir identificadores automaticamente aos conjunto de dados disponibilizados. Um identificador persistente funciona, em palavras simples, como um atalho para um programa num computador. Não é uma cópia do conjunto de dados, mas sim apenas um apontador para o local onde esse conjunto de dados está publicado (tipicamente, um repositório). Sendo assim, deve poder ser \emph{desreferenciado}, ou seja, deve ser possível obter o conjunto de dados a partir do seu identificador. Para um ser humano, tal operação pode consistir apenas num simples clique numa ligação apresentada no seu navegador Web; para uma máquina, contudo, pode ser um pedido de rede à plataforma que gere os identificadores. Em ambos os casos, a entidade solicita o recurso por detrás daquele identificador; no caso do humano, a resposta será em \gls{HTML}, código que o seu navegador pode interpretar de forma a construir  uma página web legível para humanos. Para uma máquina, o formato devolvido poderá ser, por exemplo \gls{XML} ou \gls{RDF} (formatos de representação de informação que as máquinas podem interpretar).

No caso do repositório de dados do INESC TEC\footnote{Ligação: \url{https://rdm.inesctec.pt}}, a \gls{FCT} negociou um contrato para a aquisição de pacotes de identificadores em colaboração com a DataCite\footnote{Ligação: \url{https://datacite.org/}}, que permitiu ao repositório emitir DOI para os conjuntos de dados depositados.

Para satisfazer os requisitos F2 e F3 é necessário que os registos de metadados associados a cada conjunto de dados obedeçam a um esquema normalizado, como é o caso do DataCite Schema\footnote{Ligação: \url{https://schema.datacite.org/meta/kernel-4.3/}}. Uma grande vantagem do uso de um esquema de metadados em conjunção com a atribuição de um DOI é que cada identificador ficará associado a uma ficha de metadados no momento da cunhagem do \gls{DOI}. Tipicamente, as plataformas de emissão de identificadores não permitem sequer a sua cunhagem sem o preenchimento dessa ficha de metadados. Esta ficha ficará então guardada na plataforma emissora do \gls{DOI}, e mesmo que o conjunto de dados deixe de estar disponível no repositório, a ficha continuará disponivel para consulta na plataforma fornecedora do identificador (satisfazendo o requisito A2).

O segundo princípio \gls{FAIR} impõe que os dados sejam \emph{Accessible}, pois tanto dados como metadados devem ser acessíveis através através de identificadores, através de protocolos de comunicações normalizados (A1). Este ponto requer que o protocolo seja aberto, livre e universalmente implementável (A1.1), e que inclua procedimentos de autenticação e autorização, quando necessário (A1.2). O segundo requisito para assegurar a acessibilidade (A2) é que os metadados devem permanecer acessíveis, mesmo quando os dados deixam de o estar.

Para satisfazer o requisito A1, um repositório deverá suportar o processo de desreferenciação de identificadores persistentes, quer por máquinas quer por seres humanos. Quando se fala de protocolos de comunicações normalizados, fala-se quase sempre do protocolo \gls{HTTP}. Este é talvez o protocolo mais usado na web para transferência de informação, e satisfaz o requisito A1. Ele inclui dois mecanismos relevantes para a satisfação do requisito A1.1, chamados \emph{Content Negotiation}\footnote{Ligação: \url{https://www.w3.org/Protocols/rfc2616/rfc2616-sec12.html}}, Authorization \footnote{Ligação: \url{https://www.w3.org/Protocols/HTTP/1.0/draft-ietf-http-spec.html\#Authorization}} e Access Authentication\footnote{Ligação: \url{https://www.w3.org/Protocols/HTTP/1.0/draft-ietf-http-spec.html\#BasicAA}}.

De uma forma muito breve, este mecanismo permite a um cliente (pode ser um navegador web ou um programa de computador) solicitar ao servidor (máquina que disponibiliza o registo de metadados de um determinado conjunto de dados) que envie a informação determinado formato, ao escrever esse pedido no cabeçalho do pedido \gls{HTTP} (que pode ser visto como a secção ``Destinatário'' de um envelope). Ao especificar que pretende \gls{HTML}, um navegador irá obter o código necessário para apresentar os dados sobre o conjunto de dados a um ser humano; se uma máquina solicitar \gls{XML} ou \gls{RDF}, irá obter um documento que é muito mais difícil de ler por parte de um ser humano, mas que uma máquina interpretará corretamente. A beleza deste sistema de negociação é que permite obter essas diferentes representações a partir de um mesmo identificador, mudando apenas o cabeçalho do pedido inicial.

Apesar da autenticação básica oferecida pelo protocolo \gls{HTTP} ser suficiente para satisfazer requisito A1.1, existem protocolos alternativos que podem ser adoptados caso hajam outros requisitos relevantes para a instituição que implementa um repositório de dados. Listam-se alguns protocolos abertos que podem ser relevantes neste cenário:

\begin{itemize}
  \item \textbf{Autenticação federada}: O protocolo Shibboleth\footnote{Ligação: \url{https://www.shibboleth.net/index/}} é bastante usado nas instituições de ensino superior portuguesas para controlar o acesso a multiplos recursos através do portal de autenticação federada oferecido pela \gls{FCCN}. Um repositório institucional deverá registar-se junto do Identity Provider relevante para permitir aos utilizadores autenticar-se com as mesmas credenciais que usam para aceder aos restantes recursos da sua instituição.
  \item \textbf{Autenticação delegada via outros providers}: O protocolo OAuth 2.0\footnote{Ligação: \url{https://oauth.net/2/}} permite aos utilizadores autenticar-se com as mesmas credenciais que utilizam em outros serviços como o \gls{ORCID}\footnote{Ligação: \url{https://members.orcid.org/api/oauth2}}, que pode ser útil para assegurar a autenticidade de auto-depósitos ou modificações feitas aos metadados pelos próprios criadores dos mesmos. Desta forma, podemos garantir que se uma determinada operação é realizada por alguém, se esse alguém se tiver autenticado com sucesso junto do \gls{ORCID}, por exemplo. Este mecanismo foi implementado na plataforma de gestão de dados Dendro, para agilizar o registo de novos utilizadores e a sua autenticação. Desta forma, o \gls{ORCID} de cada utilizador ficará associado ao seu perfil, e indiretamente às operações efetuadas por esse utilizador no repositório.
\end{itemize}


O terceiro princípio \gls{FAIR} diz que a gestão de dados deve ser \emph{Interoperable}. Isto deve-se ao facto dos dados precisarem frequentemente de ser integrados com outros dados, e como tal têm que ser facilmente integráveis em fluxos de trabalho de processamento. Só assim podem ser facilmente armazenados e analisados de forma interoperável. Para assegurar a interoperabilidade, tanto dados como metadados devem (I1) usar uma linguagem formal, acessível e largamente aplicada para representação de conhecimento, (I2) devem usar vocabulários que, por sua vez, seguem os princípios \gls{FAIR}, e (I3) devem incluir referências qualificadas para outros dados e metadados.

Talvez o princípio mais difícil de assegurar, a interoperabilidade implica a representação da informação constante no repositório em formatos que permitam a interpretação por sistemas externos. Isto quer dizer que os dados devem ser representados não só em formatos amigáveis como largamente suportados pelas bibliotecas de manipulação de dados mais usadas e disponíveis em código aberto. Para os dados, por exemplo, devem ser adotados os formatos livres de dependências e largamente suportados, como por exemplo \gls{XML}, \gls{CSV} ou \gls{TSV}, em detrimento de formatos binários ou proprietários. Para os metadados, devem ser representados com recurso a standards bem definidos na comunidade, sejam eles formalizados como esquemas XML, ou ontologias no caso do repositório disponibilizar os registos de metadados como \gls{LOD}.

Alguns exemplos destes esquemas incluem o amplamente utilizado Dublin Core ou o schema.org, um esquema de metadados para a web que reuniu a colaboração das principais tecnológicas (Google, Microsoft, Yahoo e Yandex)\footnote{Ligação: \url{https://schema.org/}}, que possam tanto ser formalizados como esquema \gls{XML} como sob a forma de ontologias. Sejam qual forem os vocabulários seleccionados, eles próprios devem seguir os princípios \gls{FAIR}, o que significa que devem estar livremente acessíveis e devidamente documentados.

Por último, a interoperabilidade também diz respeito à inclusão de referencias para recursos relacionados. Dessa maneira, deverão ser incluídas nos registos de metadados de cada conjunto de dados referências para materiais relacionados. Exemplos de materiais relacionados incluem por exemplo um documento de dissertação ou tese, artigos resultantes da produção dos conjuntos em questão. Os esquemas ou ontologias utilizadas para a descrição devem assim incluir descritores que permitam essas referências, como é o exemplo do descritor \texttt{references} do Dublin Core\footnote{Ligação: \url{https://www.dublincore.org/specifications/dublin-core/dcmi-terms/terms/references/}} ou o descritor \texttt{citation} do schema.org\footnote{\url{https://schema.org/citation}}.

O quarto e último dos princípios defende que os dados devem ser \textit{Reusable}, pois o objetivo final dos princípios \gls{FAIR} é fomentar a sua reutilização. Para serem Reutilizáveis, tanto dados como metadados devem (R1) ser descritos com uma grande variedade de atributos corretos e relevantes. Este princípio subdivide-se em três: (R1.1) devem ser publicados com uma licença de utilização clara e acessível, (R1.2) devem estar associados a informação de proveniência detalhada, sendo que esses dados e metadados devem satisfazer as normas relevantes de cada domínio (R1.3).

As licenças a escolher (recomendação R1.1) ficam ao critério da instituição de investigação, da entidade financiadora, e de quem cria os dados. Contudo, para a disponibilização de dados de investigação em regime de \emph{Open Data} é desejável a adoção de licenças que permitam a reutilização dos dados e metadados com o mínimo de restrições.

Para oferecer a terceiros a liberdade de reutilizar os dados em trabalhos derivados e ao mesmo tempo assegurar a atribuição de crédito aos autores, as licenças Creative Commons (nomeadamente a CC BY-4.0)\footnote{Ligação: \url{https://creativecommons.org/licenses/by/4.0/}} são geralmente adequadas para a disponibilização de conjuntos de dados produzidos em projetos financiados por dinheiros públicas.
%
Grande parte dos investigadores podem, no entanto, sentir dificuldades no momento da escolha da licença mais adequada para os conjuntos de dados que desejam publicar. É nesta altura que uma plataforma de repositório pode complementar o trabalho dos curadores, ao fornecer uma lista das licenças mais comuns incluindo descrições claras e sucintas de cada licença. Para aqueles utilizadores que pretenderem uma análise mais pormenorizada devem também oferecer uma ligação direta para a licença completa em linguagem jurídico-legal.

A recomendação R1.2 refere a necessidade de manter um historial que permita estabelecer a proveniência de um conjunto de dados. Enquanto que é relativamente simples manter um historial de modificações dentro de uma plataforma de repositório, o desafio é maior quando se considera que um conjunto de dados pode ser derivado de outro, tendo este último o seu próprio historial de modificações registado em outra plataform.

Existe aqui uma clara vantagem em usar \gls{LOD} como formato de exposição de metadados (incluindo a proveniência) neste cenário. Para tal, é necessário que os repositórios de dados ofereçam um histórico de modificações em \gls{LOD} que obedeça a ontologias como a PROV-O\cite{lebo2013prov} (mais normativa pois é uma recomendação da \gls{W3C}) ou a \gls{PAV}~\cite{ciccarese2013pav} (uma alternativa mais simples, também apelidada de \emph{lightweight ontology}). Desta forma, quando uma terceira máquina des-referenciar o conjunto de dados derivado será também interrogado o repositório onde se encontra o conjunto de dados original, e assim sucessivamente, se ele próprio também for um conjunto de dados derivado. Torna-se assim transparente e automática a recuperação de todos os dados relativos ao historial de modificações e versões de um conjunto de dados, sem necessidade de indexar todos os recursos e sem que haja lugar à intervenção humana.

É bastante claro que domínios de investigação distintos poderão gerar conjuntos de dados muito diferentes também. Como tal, o processo de descrição dos mesmos deve também ser adaptado. Satisfazer a recomendação R1.3, que preconiza que os dados e metadados devem obedecer às normas vigentes em cada domínio requer suporte por parte do software de repositório, que deve ser flexível ao ponto de permitir a parametrização de múltiplos esquemas de metadados para distintos domínios. Deverá também permitir a escolha e combinação de múltiplos descritores (genéricos e específicos do domínio do conjunto de dados em depósito) aquando do preenchimento do registo de metadados, sejam esses descritores genéricos ou específicos de domínio. É preciso também não esquecer o trabalho fundamental dos curadores no apoio institucional aos seus investigadores, pois a escolha dos descritores mais adequados a cada conjunto de dados.

% section o_papel_do_repositorio_na_aplicacao_dos_principios_fair (end)

\section{A automação dos planos de gestão de dados} % (fold)
\label{sec:a_automacao_dos_planos_de_gestao_de_dados}

Qualquer processo de gestão de dados de investigação deve começar com o desenho de um \gls{DMP}. Um \gls{DMP} é um documento tipicamente escrito e aprovado antes do início de um projeto de investigação. Em alguns casos, é já requisito obrigatório em chamadas de propostas de projetos de investigação suportados por fundos públicos~\cite{Foundation2011}) e que especifica todos os aspetos relacionados com a gestão dos dados produzidos ao longo do projeto~\cite{10.1371/journal.pcbi.1004525}.

Entre outros aspetos, o DMP especifica quais os dados a produzir no contexto de um projeto, que metadados serão anexados aos dados publicados, onde eles irão ficar disponíveis após o fim do projeto, entre outros pontos. No Reino Unido, o \gls{DCC} disponibiliza tanto um guião~\cite{jones_dmp} para a escrita deste tipo de documentos como uma ferramenta online, o DMPOnline~\footnote{\url{Ligação: https://dmponline.dcc.ac.uk}}, que assiste os investigadores e curadores na elaboração dos seus \gls{DMP}, assistidos por uma base de dados de templates de \gls{DMP} aceites por diferentes entidades financiadoras.

Com a grande quantidade de dados de investigação gerada diariamente surge a necessidade de acompanhar e monitorizar a implementação de \gls{DMP}, mas de forma automática. Três exemplos simples de operações frequentes, repetitivas e facilmente automatizáveis são:

\begin{itemize}
    \item Simples verificação da disponibilidade dos dados depositados
    \item Controlo de qualidade dos metadados associados aos conjuntos de dados acabados
    \item Verificação das licenças associadas aos conjuntos de dados
\end{itemize}

Para lidar com estas operações e outras de forma mais automática surgiram os chamados \gls{DMP} accionáveis por máquinas (\gls{maDMP}). O objetivo destes documentos é libertar os curadores de dados para outras operações menos automatizáveis, como o suporte direto aos investigadores ou atividades de formação para ajudar à curadoria de dados no dia-a-dia.

Como preconizado pelos autores das 10 regras para \gls{maDMP}~\cite{miksa_tomasz_2018_1172673}, deve ser possível aos sistemas informáticos levar a cabo ações em nome das partes interessadas no processo de gestão de dados de investigação. Operações dadas como exemplo incluem a recolha automática de informação administrativa relevante para anotação dos recursos produzidos. Essa informação inclui a recuperação de referências aos financiadores dos projetos, os currículos dos autores dos conjuntos de dados ou as informações corretas das instituições de acolhimento dos investigadores.

Esta necessidade de suporte a automação e integração tem implicações vastas do ponto de vista do desenho e funcionalidades de um repositório de dados, na medida em que a solução deve disponibilizar uma \gls{API} completa e bem documentada, de forma a permitir a sistemas externos executar estas operações sem a necessidade de intervenção humana.

\subsection{O papel da interoperabilidade na descoberta de conjuntos de dados} % (fold)
\label{sub:o_papel_da_interoperabilidade_na_descoberta_de_conjuntos_de_dados}

Os agregadores de repositórios são portais que indexam o conteúdo dos repositórios para facilitar a pesquisa e descoberta de recursos neles contidos. Exemplos destes agregadores são, por exemplo, o portal re3data da DataCite~\footnote{Ligação: \url{https://www.re3data.org/}} ou o Dataset Search da Google\footnote{Ligação: \url{https://datasetsearch.research.google.com/}}. Para facilitar a integração com diretórios de dados, o software de repositórios devem ser assentes em protocolos de interoperabilidade standard.

Para facilitar a indexação dos conteúdos dos repositórios, estes implementam suporte ao protocolo OAI-PMH (Open Access Initiative Protocol for Metadata Harvesting). Este protocolo expõe os metadados de todos os registos no repositório de forma paginada, para que seja lida sequencialmente e indexada; contudo, não possibilita a pesquisa e recuperação de registos específicos por termos contidos nos registos de metadados, por exemplo. Para conseguir tal funcionalidade é necessário proceder à inserção de todos os documentos num índice de pesquisa como o Lucene ou Solr. Uma consequência óbvia é a necessidade de manutenção de cópias dos registos e atualizações periódicas de todo o índice---uma operação lenta e dispendiosa em termos de recursos informáticos, que coloca todo o custo de manutenção do serviço do lado dos clientes.

Uma situação oposta ocorre com a adoção de \gls{LOD} na representação de metadados para conjuntos de dados de investigação. Esta representação de dados vem eliminar a necessidade de indexação periódica de conteúdos e melhorar muito a precisão das pesquisas, caso os dados sejam estruturados de acordo com uma ou mais ontologias. Uma ontologia é definida, na Ciência da Informação, como uma \textit{especificação de uma conceptualização}\cite{gruber1995toward}, que permite às máquinas interpretar o significado das entidades representadas num sistema e das relações existentes entre elas.

Uma representação via \gls{LOD} permite uma maior descentralização da carga de pesquisas sobre os servidores onde os repositórios estão alojados. Reduz também a necessidade de indexação porque é possível executar pesquisas nos próprios servidores sem ter que primeiro indexar todo o seu conteúdo e permite aos clientes do serviço recuperar conjuntos de dados com muito mais precisão do que nos casos em que se usa um índice de pesquisa simples baseado em palavras-chave, pois é possível especificar critérios muito detalhados sob a forma interrogações \gls{SPARQL}.

A disponibilização de \gls{LOD} por parte dos repositórios tem, por contraponto aos benefícios de interoperabilidade e abertura, um custo para quem os hospeda. As consultas \gls{SPARQL} são exigentes do ponto de vista computacional e, se um número elevado de utilizadores (máquinas ou humanos) tentarem ao website ao mesmo tempo, rapidamente autmentarão os tempos de resposta da infraestrutura. Isto é especialmente relevante quando se considera a funcionalidade de pesquisa federada, onde múltiplos servidores podem ser contactados para conseguir obter os resultados pretendidos por uma única pesquisa iniciada em qualquer dos nós da federação.

O futuro dos repositórios de dados pode passar então por uma solução híbrida, onde servidores e clientes partilham informação para possibilitar também uma partilha do esforço de interrogação e descoberta de dados. Soluções como a Linked Data Fragments\cite{VERBORGH2016184} propõem a combinação de interrogações sobre \gls{SPARQL} \textit{endpoints} tradicionais (no servidor do repositório que expõe \gls{LOD}) com a interrogação a dados locais a cada cliente, reduzindo assim a carga sobre os servidores.

% subsection o_papel_da_interoperabilidade_na_descoberta_de_conjuntos_de_dados (end)

\subsection{Repositórios como plataformas de computação} % (fold)
\label{sub:repositorios_como_plataformas_de_computacao}

Cada vez mais, o valor de um repositório de dados reside na facilidade de reutilização dos dados nele contidos. Tecnologias recentes como os Research Notebooks e a Containerização permitem tornar os algoritmos mais portáteis, de forma a poderem ser executados remotamente sobre os dados depositados num repositório. O papel do repositório torna-se assim muito mais do que um armazém combinado com um motor de busca sobre conjuntos de dados.

Os Research Notebooks são cadernos de laboratório electrónicos que combina num único pacote os dados-base e o código de processamento desses dados. Esse código pode servir, por exemplo, para transformar os dados base em dados processados, e até mesmo para produzir visualizações e gráficos em tempo real~\cite{8760672}.

Os Notebooks vêm também possibilitar suportar um novo tipo de publicação científica, fortemente suportada por dados e pela Internet. A combinação elegante de texto, trechos de código e visualizações apelativas permitiu a criação dos chamados ``Web Journals'' como por exemplo o distill.pub\footnote{Ligação: \href{https://distill.pub/}}. Estes distinguem-se das publicações convencionais ao embutir visualizações interativas no texto dos artigos científicos, fomentando a experimentação através da manipulação de parâmetros de entrada dos algoritmos que geram essas visualizações, para produzir um feedback visual imediato aos leitores. Toda a computação tem que ser executada em tempo real, o que implica a montagem de uma réplica do ambiente de processamento por detrás do portal web que suporta o journal.

Com o empacotamento do processamento e dos dados a ser abordado pelos Research Notebooks, surge a necessidade de optimizar a localização da computação e dos dados, para aproximar um do outro. Nem todos os conjuntos de dados se prestam a ser transportados pela rede em tempo útil para a execução de um determinado algoritmo, pois em certas disciplinas o seu tamanho pode ascender a centenas de giga ou terabytes.

% TODO Revisão JRS aqui

Torna-se necessário levar a computação até aos dados em vez de ter que transmitir os dados até ao local onde se realiza a computação. Este tipo de computação que ocorre junto dos dados é um paradigma é implementado em portais de computação como o D4Science~\footnote{Ligação: \url{https://www.d4science.org}} ou o EUDAT B2Stage\footnote{Ligação: \url{https://www.eudat.eu/b2stage}}. Com a esperada descentralização e interoperabilidade que se espera dos repositórios de dados defendida pelo \gls{GEI} da \gls{EOSC}, espera-se que cada repositório seja capaz de oferecer capacidade de computação local a pequena escala e junto das fontes de dados. Este cenário é mais próximo do \emph{edge computing}, um termo mais vulgarmente associado às aplicações IoT. Por contraponto ao \emph{cloud computing}, onde todos os recursos se encontram na nuvem e os dados têm que ser cuidadosamente aproximados dos nós de computação (mesmo em termos geográficos) para reduzir tráfego na rede, o \emph{edge computing} defende que cada nó da rede deve ter uma pequena mas importante capacidade de computação para executar operações à medida que os dados são produzidos ou atualizados.

Quando se fala de distribuição de computação e reprodutibilidade, surge imediatamente a necessidade de replicar também todo o ambiente de execução no qual essa computação é executada. Sem uma solução de virtualização, isso implica a instalação de um sistema operativo e de todas as dependências relevantes para a execução do código relevante. Este é um processo manual e dispendioso que torna claramente necessário assegurar a portabilidade do ambiente de execução, tornando-o instalável automaticamente em qualquer nó com o mínimo de dependências.

A containerização é uma tecnologia de virtualização que, quando comparada com as máquinas virtuais convencionais, elimina a necessidade de virtualizar um sistema operativo para executar um determinado programa, substituindo essas dependências por um ambiente de execução (ou \textit{runtime}) próprio. O exemplo mais comum de ambiente de virtualização é o Docker\footnote{Ligação: \url{https://www.docker.com/}}. Esta eficiência torna possível a um repositório de dados executar algoritmos directamente sobre os dados nele depositados e retornar apenas os resultados aos clientes que pedem esses processamentos, em vez de exigir a descarga dos dados para o cliente. Talvez a funcionalidade mais interessante para um repositório de dados é a possibilidade de especificar todos os passos de instalação de dependências necessárias à execução dos algoritmos de processamento de dados, reduzindo em larga medida os problemas de replicação do ambiente de execução de determinado algoritmo.

% subsection repositorios_como_plataformas_de_computacao (end)

\subsection{A \emph{blockchain} e as suas aplicações aos Dados Abertos} % (fold)
\label{sub:a_emph_blockchain_e_as_suas_aplicacoes_aos_dados_abertos}

Num futuro de repositórios de dados descentralizados e interligados num ambiente descentralizado será necessário conceber soluções para assegurar a autenticidade da informação colocada em cada nó da federação. Em particular, a criação de um \emph{ledger} de transações distribuído, como por exemplo uma \emph{blockchain} envolvendo os repositórios dda federação, reduz a dependência de entidades terceiras---como os emissores de certificados---para assegurar a autenticidade dos dados. Este tema já está a ser alvo de investigação, com o objetivo de descobrir formas de utilização da blockchain para assegurar a validade dos resultados de operações sobre dados abertos~\cite{10.1007/978-3-030-23946-6_28}. É portanto claro o potencial de aplicação deste tipo de tecnologias aos diversos aspetos da gestão de dados abertos, destacando-se dois pontos:

\begin{itemize}
	\item \textbf{Proveniência}: Através de um registo distribuído de modificações feitas a dados e metadados será possível assegurar a sua autenticidade e não-repúdio relativamente à autoria de quaisquer operações realizadas sobre os dados. Esta prática deve estar presente antes, durante e após a publicação dos conjuntos de dados; quando aplicada antes da publicação, permite rastrear todas as modificações feitas ao longo do tempo, para reduzir situações de \emph{p-hacking}\cite{10.1371/journal.pbio.1002106}, por exemplo. Após a publicação, um registo distribuído de alterações permite aos autores continuar a modificar um conjunto de dados após a publicação do conjunto de dados considerado ``final'', de forma completamente transparente. Uma outra vantagem é a resiliência do sistema, pois mesmo que o repositório onde um conjunto de dados está publicado seja desligado, esse historial de modificações continuará registado nos restantes nós da \emph{blockchain}.
	\item \textbf{Fiabilidade de resultados}: Regra geral, um valor só é registado na \emph{blockchain} se existir um consenso na rede de computadores que colaboram nessa \emph{blockchain}. Este mecanismo pode ser usado para atestar a reprodutibilidade das operações de transformação realizadas sobre um conjunto de dados. Neste caso em particular, só após um determinado número de nós (dependendo da política em vigor na \emph{blockchain}) levarem a cabo as mesmas operações e confirmarem os resultados é que é criado o \emph{consenso} necessário à escrita para a \emph{blockchain}, passando aquela operação a fazer parte do historial de operações sobre aquele conjunto de dados.
\end{itemize}

% subsection a_emph_blockchain_e_as_suas_aplicacoes_aos_dados_abertos (end)

% section a_automacao_dos_planos_de_gestao_de_dados (end)

\section{Comparação de funcionalidades} % (fold)
\label{sec:comparacao_de_funcionalidades}

Existem diferentes alternativas para a montagem de um repositório de dados. Uma análise comparativa produzida pelo projeto Dataverse\footnote{Ligação:  \url{https://dataverse.org/blog/comparative-review-various-data-repositories}} apresenta uma comparação das principais plataformas de repositórios de dados, tendo em conta três grupos de facetas: Funcionalidades de Software, Controlo/Organização e Conteúdo. Uma outra comparação~\cite{amorimComparison2017} discute em mais profundidade algumas características técnicas de diversas plataformas. Esta análise foca-se apenas nas alternativas disponíveis em regime de código aberto e instaláveis localmente, de forma a poder retirar conclusões relativamente ao modelo de dados por detrás de cada solução de software.

Tão importante como comparar funcionalidades oferecidas pelas plataformas é estudar quais funcionalidades são mais valorizadas pelos utilizadores destas plataformas. Para responder a esta questão, o Grupo de Interesse das Plataformas de Gestão de Dados de Investigação (RPRD IG) da Research Data Alliance (RDA) compilou uma Matriz de Casos de Uso e Requisitos Funcionais para as Plataformas de Gestão de Dados de Investigação. Esta matriz resultou de um inquérito a 11 grupos que apresentaram os seus casos de uso para repositórios de dados de investigação. Cada um destes grupos classificou a importância de 26 requisitos vulgarmente associados a um repositório de dados. No final, foi atribuída uma pontuação a cada um dos requisitos funcionais, com base na importância que a totalidade dos grupos lhe atribuiu no contexto do seu caso de uso.

De forma a comparar de forma sucinta algumas das plataformas mais utilizadas, é necessário considerar não só a quantidade e variedade de funcionalidades como também a sua importância para a maioria dos utilizadores. Assim, na Figura~\ref{comparacao_repos} apresenta-se não só uma pontuação de funcionalidades, como também algumas das principais características dessas plataformas. Nesta figura, apresenta-se primeiramente um gráfico de barras que ordena as plataformas de acordo com uma ``Pontuação de funcionalidades''. Esta foi determinada associando manualmente os requisitos funcionais da RDA às características listadas na comparação de repositórios apresentada pela DataVerse, e calculando o produto de cada relação estabelecida pela importância dada ao requisito pela RDA. Desta forma, foi possível atribuir diferentes \emph{pesos} às características desta última comparação~\footnote{Os dados base deste estudo estão disponíveis no repositório GitHub deste relatório, como materiais auxiliares. Ligação: \url{https://github.com/silvae86/repositories-paper-eda}}.

\begin{figure*}[h!t!]
\centering
\includegraphics[width=0.95\textwidth]{Sources/DataverseComparativeReview/plot.pdf}
\caption{Análise comparativa de diferentes plataformas de \emph{software} para repositórios de dados}
\end{figure*}

A figura apresenta também uma tabela com algumas características mais relevantes de cada plataforma:

\begin{itemize}
	\item \textbf{Código Aberto}\\
	Na primeira linha distinguem-se as soluções que estão disponíveis em regime de código aberto daquelas em que não é o caso. O facto do código de uma solução estar disponível em acesso aberto tem inúmeras vantagens, desde que haja uma comunidade disposta a manter esse software. Exemplos são uma maior segurança, pois o facto do código ser aberto facilita a correção rápida de falhas de segurança por parte da comunidade, ao invés de ter que esperar pela correção por parte da equipa de uma empresa. Qualquer pessoa tem também a liberdade de modificar e melhorar o software para satisfazer uma determinada necessidade, sendo que esses melhoramentos poderão potencialmente ser incluídos no tronco comum da solução, caso se verifique que o requisito se verifica em outras comunidades e os desenvolvedores assim o entenderem. Esta flexibilidade pode proporcionar um ritmo e velocidade de desenvolvimento mais elevado do que as soluções proprietárias, que têm equipas de desenvolvimento limitadas; contudo, a própria comunidade de desenvolvimento é responsável pela manutenção da qualidade do código. O modelo de código aberto não invalida a hipótese das entidades que suportam um repositório realizarem alterações pagas ao código, num cenário semelhante à consultoria de implementação do repositório. Nestes casos, o ``produto'' em si não é o software, mas sim o serviço de instalação e personalização da solução de código aberto numa determinada instituição. De qualquer forma, o código desenvolvido no contexto destas implementações pode também ser incluído no tronco comum, beneficiando toda a comunidade que utiliza esse software. É também normal encontrar nos projetos de software open-source um conjunto de fóruns públicos onde os utilizadores podem propor melhorias ao software e reportar \textit{bugs}.
	\item \textbf{Grátis}
	Quase todas as soluções em análise são completamente grátis ou oferecem um nível de serviço base grátis. Os sistemas que funcionam com base na cloud incluem normalmente uma capacidade de armazenamento de dados limitada, sendo que os utilizadores podem adquirir um pacote de armazenamento maior se necessitarem. As soluções que podem ser instaladas localmente apenas dependem da capacidade de armazenamento do servidor no qual são instaladas.
	\item \textbf{Número de conjuntos de dados, ficheiros e utilizadores}
	O número de recursos geridos pelas plataformas e o número de utilizadores que confiam nessas plataformas para a sua gestão de dados é uma importante métrica da sua maturidade.
	\item \textbf{Certificação}
	A certificação de repositórios continuará a assumir relevância aquando da adoção de uma solução de repositório. No caso das soluções SaaS, a certificação do repositório pode beneficiar todas as instituições que contratem os serviços dessas plataformas, pois todos os seus investigadores poderão mencionar esse aspeto na escrita dos DMP que anexam às suas propostas de projetos. As soluções SaaS têm portanto todo o interesse em procurar a certificação para oferecer mais valor aos seus clientes. No caso das alternativas instaladas e mantidas localmente, as funcionalidades da solução de software em particular não são condição suficiente para a certificação. Contudo, as funcionalidades da plataforma a seleccionar não devem impedir a futura certificação do repositório.
	\item \textbf{Infraestrutura}
	Esta categoria distingue as plataformas que só estão disponíveis como SaaS daquelas que podem também ser instaladas e geridas localmente.
\end{itemize}

Por fim, é necessário considerar o modelo de hospedagem ou infraestrutura do repositório, que está geralmente ligado ao modelo de negócio das entidades que o suportam. Neste domínio, existem as soluções SaaS\footnote\emph{{Software as a Service}}, que podem ser vistos como ``chave-na-mão''. Nestes modelos, toda a infraestrutura e serviços por detrás da página web do repositório (alojamento, manutenção, assistência técnica, curadoria ou atribuição de identificadores) faz parte de um pacote oferecido às instituições interessadas nesse repositório. Em alternativa, apresentam-se as soluções alojadas localmente, que diferem das soluções SaaS na medida em que exigem instalação e manutenção por parte de uma equipa de TI interna à organização. Da mesma forma, o trabalho de curadoria na plataforma também tem que ser levado a cabo inteiramente pelos elementos da organização e geralmente não existe nenhuma obrigação de assistência por parte dos desenvolvedores do software.

% section comparacao_de_funcionalidades (end)

\section{Certificação} % (fold)
\label{sec:certificacao}

Para além dos princípios FAIR, que se apresentam como linhas gerais para aquilo que um processo de gestão de dados deve proporcionar, surgiram recentemente os princípios TRUST~\cite{TRUSTprinciples}. Estes focam-se no problema da confiança nas infraestruturas, organizações e qualidade dos dados de investigação. Estes princípios oferecem, de acordo com os autores, uma \emph{framework} de discussão para ajudar a melhorar a confiança de todas as partes interessadas no processo de gestão de dados. Estas partes interessadas vão desde os próprios investigadores, passando pelas instituições académicas ou de ciência que os acolhem, as entidades financiadoras, entre outros~\cite{DCC_stakeholders}.

De acordo com o Guião Estratégico de Implementação (GEI) da EOSC (European Open Science Cloud), a certificação de repositórios de dados é uma parte essencial na visão a longo prazo da Comissão Europeia. Os processos de certificação são exaustivos e requerem não só excelência técnica como também um compromisso a longo prazo das instituições que os suportam.

De um ponto de vista mais económico e de soberania dos dados, a certificação de repositórios assumirá um papel cada vez mais importante no funcionamento de uma instituição académica ou de investigação. Existem diversas razões, de entre as quais se destaca o crescente número de entidades financiadoras e publicações de renome que exigem a disponibilização dos dados-base e do código-fonte que sustenta as publicações. Neste cenário, a qualidade dos repositórios onde tais recursos são disponibilizados terá necessariamente que ser elevada. Desta forma, a certificação tornar-se-à assim uma pedra fundamental no suporte aos projetos de investigação, pois as instituições terão que assegurar aos seus investigadores a satisfação dos presentes e futuros requisitos de financiadores aquando da escrita dos planos de gestão de dados a anexar às propostas de financiamento. Essas instituições terão então duas opções para garantir os requisitos: ou embarcam num processo de certificação dos seus próprios repositórios institucionais, ou terão que subcontratar o processo de depósito e curadoria de dados a um repositório certificado. Levada a cabo a certificação, contudo, o repositório poderá tornar-se uma fonte de receita para as instituições que os suportam, pois oferece um valor acrescentado caso essas instituições queiram oferecer um serviço de preservação a longo prazo como um serviço~\cite{Lindlar_Schwab_2019}.

A certificação é um processo custoso e demorado que constitui um projeto por si só. A título de exemplo, o TIB Leibniz Information Centre for Science and Technology, que reportou os resultados das suas certificações DSA e nestor em 2018; os números são apresentados na Tabela~\ref{tab:tib_certification_numbers}.

\begin{table}
    \caption{Recursos dispendidos na certificação do TIB, para o DSA V.2 e nestor Seal, números~\cite{Lindlar_Schwab_2019}}\label{tab:tib_certification_numbers}
    \centering
    \scriptsize
    \begin{tabular}{|c|c|c|}
	\hline
    \textbf{Métrica} & \textbf{DSA} & \textbf{nestor Seal}\\
	\hline
	\hline
	Fee for process & 0 €  & 500€ \\
    Project Duration (meses) & 9 & 12 \\
	Pessoas-Mês\footnote{Calculado com base nas regras UE FP5, de 8 horas por dia e 17,5 dias por mês} & 3.7 & 11\\
	Pessoas envolvidas & 7 & 16\\
	Unidades organizacionais envolvidas & 5 & 8\\
	\hline
    \end{tabular}
    \scriptsize
\end{table}

\subsection{Data Seal of Approval} % (fold)
\label{sub:data_seal_of_approval}

O Data Seal of Approval (DSA)~\footnote{Ligação: \url{www.datasealofapproval.org}} foi uma iniciativa de certificação de repositórios desenvolvida em 2008 pelo holandês Data Archiving and Networked Services (DANS). Consiste num selo que garante que que dados arquivados num repositório DSA podem ser encontrados, interpretados e usados no futuro. De 2008 a 2018, os requisitos para obtenção do selo passaram por 3 revisões. O DSA foi fundido em 2018 com o CoreTrustSeal, e os repositórios que receberam o DSA para o período 2014-2017 têm que passar novamente pelo processo de certificação para o CoreTrustSeal. Uma lista dos repositórios que obtiveram o DSA durante a sua existência está também disponível~\footnote{Ligação: \url{https://easy.dans.knaw.nl/ui/datasets/id/easy-dataset:116038}}.

% subsection data_seal_of_approval (end)

\subsection{nestorSEAL} % (fold)
\label{sub:nestorseal}

A nestor (network of expertise in long-term storage of digital resources in Germany) envolve 23 grandes instituições alemãs de investigação e ciência e deu origem a 12 grupos de trabalho. Um grupo de trabalho especializado em certificação mantém o nestorSEAL, uma framework de certificação que garante não só os requisitos do DSA mas também uma certificação mais extensa~\footnote{Ligação: \url{https://www.langzeitarchivierung.de/Webs/nestor/EN/Services/nestor_Siegel/nestor_siegel_node.html}}.

% subsection nestorseal (end)

\subsection{ISO 16363:2012} % (fold)
\label{sub:iso_16363_2012}

A certificação ISO 16363:2012 assume-se como a mais exigente e valiosa, mas também a dispendiosa e demorada de obter. Nem todos os repositórios devem procurar esta certificação, pois como diz o Implementation Guide da EOSC, ``um repositório deve procurar o nível de certificação apropriado e alcançável''\cite{hodson2018turning}. A certificação ISO é um processo considerado ``heavyweight'' para a maioria dos repositórios, e até 2018 apenas o National Cultural Audiovisual Archives, na Índia, havia atingido esse objetivo~\cite{Lindlar_Schwab_2019,IndianMinistryofCulture2017}.

Uma auditoria interna nacional realizada em 2015 analisou 24 repositórios alojados pelo SARI\footnote{Serviço de Alojamento de Repositórios Institucionais} do RCAAP\footnote{Repositório Científico de Acesso Aberto de Portugal}, de acordo com as três dimensões da norma: infraestrutura organizacional, gestão de objetos e infraestrutura de gestão e segurança.

Durante esta auditoria, e dado que a ISO 16236:2012 não especifica uma escala de maturidade para o cumprimento dos requisitos nela especificados, foi adotada uma escala baseada no modelo ECM3~\cite{Katuu2013}. A auditoria concluiu que a média do grau de conformidade dos repositórios analisados foi de 2.0 de um máximo de 5.0, tendo o ponto mais fraco sido a Sustentabilidade Financeira, com uma pontuação média de 1.39, e o ponto mais forte sido a Estrutura Governativa e Viabilidade Organizacional, com uma pontuação de 2.33~\cite{Carvalho2014}.

Um aspeto que distingue esta certificação da DSA, do nestorSEAL e do CoreTrustSeal é que a única instituição que a fornece, a PTAB\footnote{Primary Trustworthy Digital Repository Authorisation Body}, não obriga as instituições responsáveis pelo repositório certificado a publicar os relatórios de certificação. Consequentemente, até 2018 não havia um relatório sobre o processo de certificação 16363:2012 de nenhum repositório disponível para análise~\cite{Lindlar_Schwab_2019}.

% subsection sub:iso_16363_2012 (end)

\subsection{European Framework for Audit and Certification of Digital Repositories} % (fold)
\label{sub:european_framework_for_audit_and_certification_of_digital_repositories}

A EFACDR (European Framework for Audit and Certification of Digital Repositories) surgiu após a assinatura de um memorando\footnote{Ligação: \url{http://www.trusteddigitalrepository.eu/Memorandum\%20of\%20Understanding.html}} por parte das entidades responsáveis pelo DSA, o CCSDS\footnote{Consultative Committee for Space Data Systems}, e o standard ISO 16364:2012 e também o grupo de trabalho responsável pela DIN\footnote{Deutsches Institut für Normung} 31644.

Esta framework propõe 3 passos de certificação: a certificação básica de acordo com o DSA, uma ``Extended Certification'' que inclui uma auditoria feita pela própria instituição e avaliada externamente baseada na ISO 16363 ou na DIN 31644. Por fim, o terceiro passo é certificação formal com base nas mesmas normas, mas levada a cabo através de auditoria e certificação externa~\cite{Lindlar_Schwab_2019}.

% subsection european_framework_for_audit_and_certification_of_digital_repositories (end)

\subsection{CoreTrustSeal} % (fold)
\label{sub:core_trust_seal}

O CoreTrustSeal, lançado em 2017, resulta do trabalho do DSA e do Repository Audit and Certification DSA–WDS Partnership Working Group da RDA.
%
Este processo de certificação é citado nas Recomendações 9 e 13 do relatório da Comissão Europeia para a implementação dos princípios FAIR~\cite{hodson2018turning} como um método de certificação certificado pela comunidade que deve ser usado como base para a avaliação e certificação de serviços FAIR. Tendo em conta estas referências, e caso não existam requisitos dos stakeholders que obriguem á procura de uma certificação ISO, esta deve ser a certificação que a maioria dos repositórios institucionais deve procurar obter, pelo seu equilíbrio entre prestígio e esforço na sua obtenção.
%
Á data de escrita deste artigo existem 96 repositórios certificados de acordo com o CoreTrustSeal~\footnote{Ligação: \url{https://www.coretrustseal.org/why-certification/certified-repositories/}}, entre os quais se destaca o Portulan CLARIN a nível nacional~\footnote{Ligação: \url{https://portulanclarin.net/ecosystem/\#certification}}.

% subsection core_trust_seal (end)

% section certificacao (end)

\section{Discussão e conclusões} % (fold)
\label{sec:discussao_e_conclusoes}

% Descentralização
% Versionamento de datasets
% Proveniência
% Metadados diversos e específicos do domínio
% APIs extensas para suportar operações de machine actionable dmps
% Computação


Os princípios \gls{FAIR} para a gestão de dados de investigação, e mais recentemente os princípios TRUST, assumem-se como as linhas orientadoras para o desenho de fluxos de trabalho de gestão de dados, nos quais o repositório se assume como a pedra fundamental. Enquanto que os primeiros se focam nas boas práticas necessárias para a reutilização de conjuntos de dados, os segundos orientam a construção e manutenção de repositórios de dados confiáveis.

Os repositórios têm vindo a assumir um papel cada vez mais importante na divulgação de ciência. Enquanto arquivos de dados de investigação, o seu valor é cada vez mais medido pela sua capacidade de fomentar a reutilização dos recursos neles depositados. Desta forma, o simples armazenamento e descrição dos documentos neles depositados não chega. Os repositórios devem permitir a publicação de dados e metadados de forma interoperável, de forma a suportar a descoberta desses recursos, tanto por seres humanos como por máquinas, de forma automática.

Para além da descoberta e interrogação automática dos seus conteúdos por parte de sistemas externos, os repositórios devem oferecer uma interface de interação programática---ou API---completa. Só desta forma poderão suportar a execução de Planos de Gestão de Dados Acionáveis por Máquinas, ou \gls{maDMP}. Estes modelos vão além dos documentos convencionais, ao servirem de especificação legível, auditável e executável por máquinas das práticas a seguir durante a gestão de dados. Por exemplo, um \gls{maDMP} pode especificar que determinados conjuntos de dados devem estar disponíveis, devem incluir um DOI nos seus identificadores e os seus metadados devem obedecer a determinada norma: o repositório onde esses dados estiverem depositados tem que ser capaz de responder com essa informação---via API---quando o sistema externo encarregado de assegurar o cumprimento do \gls{maDMP} o interrogar sobre a presença desses elementos.

A capacidade de versionamento consequente auditoria transparente às alterações feitas a um conjunto de dados é também uma funcionalidade essencial nos repositórios atuais. Ambas são pré-requisito para a rastreabilidade desses dados, permitindo não só a sua evolução contínua---mesmo após a publicação como anexo a um artigo científico. Ao mesmo tempo, este registo permite atribuir aos autores dessas modificações o devido crédito.

A atribuição de crédito pelo trabalho de produção e descrição correta de conjuntos de dados é uma das questões mais relevantes para a motivação dos investigadores em todo este processo. As plataformas de repositório desempenham aqui também um papel essencial, pois são elas que devem manter a informação de quem criou ou modificou cada conjunto de dados e seus metadados que permitirá o cálculo de uma métrica de mérito. A inclusão dessa métrica de publicação nos critérios de avaliação institucional dos investigadores torna-se assim tecnicamente possível.

A oferta de capacidades de computação sobre os dados depositados num repositório requer a ligação entre os dados e plataformas de computação como por exemplo os Jupyter Research Notebooks. Desta forma, o repositório guardará não só os dados mas também os processos de análise desses dados que sustenta às conclusões publicadas. Torna-se também mais fácil para terceiros re-executar esses passos, pois reduz-se o número de dependências a instalar para recuperar o seu contexto de execução.

Por último, e em jeito de conclusão, a tecnologia desempenha um papel essencial no suporte à reprodutibilidade dos resultados apresentados nas publicações. Contudo, essa mesma tecnologia não pode nunca substituir o papel dos curadores de dados. O investimento na implementação de um repositório deve assim ser alicerçado num igual investimento na formação dos responsáveis pela gestão dos dados de investigação. São esses peritos que levam a cabo o suporte aos investigadores e que conseguem dessa forma criar um clima de confiança no processo de gestão de dados e dessa forma salientar a proposta de valor da plataforma de repositório que o sustenta.

% section discussao_e_conclusoes (end)

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
O autor agradece o convite para participar no grupo de trabalho para a Estratégia para os Dados Abertos, e convida todos os leitores deste artigo a submeter revisões e correções no repositório GitHub deste artigo~\footnote{Ligação: \url{https://github.com/silvae86/repositories-paper-eda}}.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{referencias}

%%
%% If your work has an appendix, this is the place to put it.
% \appendix

\printglossary[type=\acronymtype]
\printglossary

\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
